{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "## Project  Navigation: REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/7ad5cdff66f7229c4e9822882b3c8e57960dca4e/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f766964656f2e756461636974792d646174612e636f6d2f746f706865722f323031382f4a756e652f35623165613737385f726561636865722f726561636865722e676966\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This report provides a description of the implementation for the Deep Reinforcement Learning Nanodegree Project 2, where I had to train an agent to perform continuous control action on a double jointed robot arm to reach and track a goal. Please refer to the [README.md]() on this repository for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithm \n",
    "\n",
    "1. ### The Agent\n",
    "\n",
    "    The agent architecture can be found on \"**ddpg_agent.py**\" . This file implements an \"Agent\" class that holds:\n",
    "\n",
    "    * args (class defined on the notebook): A set of parameters that will define the agent hyperparameters\n",
    "    * state_size (int): dimension of each state\n",
    "    * action_size (int): dimension of each action\n",
    "    \n",
    "     The agent uses an actor-critic DDPG algorithm. Some of the highlights of this algorithm are.\n",
    "    * Actor critic network architecture\n",
    "    * off-policy (slower convergence but more stable)\n",
    "    * supports continous observation space and continous actions space.\n",
    "    * Uses random experience replay\n",
    "    * Uses multiple environment to parallely populate the replay buffer and thus train faster\n",
    "    * Uses double network (for actor/critic each). One for target and one for learning. Soft update is used to \n",
    "      slowly blend the target to the local model. \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. ### The Policy network \n",
    " \n",
    " Also described in the README.md file.\n",
    " \n",
    "  **Actor**    \n",
    "    - Hidden: (input, 256)  - ReLU\n",
    "    - Hidden: (256, 128)    - ReLU\n",
    "    - Output: (128, 4)      - TanH\n",
    "\n",
    "  **Critic**\n",
    "    - Hidden: (input, 256)              - ReLU\n",
    "    - Hidden: (256 + action_size, 128)  - ReLU\n",
    "    - Hidden: (128, 128)  - ReLU\n",
    "    - Output: (128, 1)                  - Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ### Hyper-parameters\n",
    "\n",
    "    As described in (more details) the readme file the following hyper parameters are used in this algorithm.\n",
    "\n",
    "  - Learning Rate: 1e-4 (in both DNN actor/critic) # learning rate \n",
    "  - Batch Size: 128     # minibatch size\n",
    "  - Replay Buffer: 1e5  # replay buffer size\n",
    "  - Gamma: 0.99         # discount factor\n",
    "  - Tau: 1e-3           # for soft update of target parameters\n",
    "  - Ornstein-Uhlenbeck noise parameters (0.15 theta and 0.2 sigma.) # Noise use to introduce entropy in the system to explore more\n",
    "  - n_episodes Size: 1000     # Maximum number of episodes for which training will proceed\n",
    "  - checkpoint_score: 30     # if the score is greater than this threshold, network is checkpointed and training is finished. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Three different agent has been trained. Please check the Navigation.ipynb notebook file for more details.\n",
    "\n",
    "**DDPG agent with random replay buffer. v2 of the Reacher Env \n",
    "\n",
    "With use of parallel environments the convergence is much much faster\n",
    "\n",
    "<img src=\"ddpg1.png\">\n",
    "\n",
    "The actor and critic models are checkpointed in<br>\n",
    "./MultiEnvCheckPt/Episode119_actor.pth<br>\n",
    "./MultiEnvCheckPt/Episode119_critic.pth<br>\n",
    "\n",
    "I initially tried the v1 of the environment (single). The convergence was so much slower (almost 1/5 th) that I decided to try out this parallel environment version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "All this results and conclusions suggest a series of changes (Future Work) to improve the agent's performance and to reduce it's instability. Future work will include \n",
    "\n",
    "* Implement D4PG algorithm [5]\n",
    "* To implement policy based algorithms \n",
    "* A Distributional Perspective on Reinforcement Learning [7][8]. \n",
    "* Study different and more complex NN' architectures applicable to the problem.\n",
    "* Auto tune hyper paramaters and NN architectures \n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "* [1] DPG: Deterministic Policy Gradient Algorithms  (http://proceedings.mlr.press/v32/silver14.pdf))\n",
    "* [2] DDPG:Human-level control through deep reinforcement learning (https://www.nature.com/articles/nature14236)\n",
    "* [3] Deep Reinforcement Learning with Double Q-learning (https://arxiv.org/abs/1509.06461)\n",
    "* [4] Prioritized Experience Replay (https://arxiv.org/abs/1511.05952)\n",
    "* [5] D4PG: DISTRIBUTED DISTRIBUTIONAL DETERMINISTIC POLICY GRADIENTS (https://arxiv.org/pdf/1804.08617.pdf)\n",
    "* [6] Reinforcement Learning: An Introduction (https://s3-us-west-1.amazonaws.com/udacity-drlnd/bookdraft2018.pdf)\n",
    "* [7] A Distributional Perspective on Reinforcement Learning (https://arxiv.org/abs/1707.06887)\n",
    "* [8]. Ray -rllib - A distributed framework for RL and hyperparameter tuning (https://ray.readthedocs.io/en/latest/rllib.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
